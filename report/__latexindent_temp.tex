\chapter{Discussion}
\label{cha:discussion}


This chapter analyse the results obtained and the method used, both described in respective chapter.


\section{Results}
\label{sec:discussion-results}


The results from training and testing the models are closely related to the results seen in related work, described in chapter \autoref{cha:theory}. 
The K-Nearest Neighbors (KNN) model was not used in any of the referenced works, but was included since it is a common ML model and results in non-linear classification borders. 
It was unexpected to see that KNN performed so unbalanced, classifying negative reviews so well, but very poor on positive reviews. 
Also the potential of Random Subspaces (RS) is hinted, it is used in a poorly parametrized way, but the generalization i.e. performance on test data is very good considering. 
However, since it uses SVM as base model it should still be close to the Linear SVM model that is a more basic model than the SVM used in RS. 


The majority voting increased performance as expected from referenced papers in a similar way. 
It could be argued that this increases complexity over just using one model, but it also balanced the FP and FN classifications. 
Possibly using majority voting the classifications will be more stable over different datasets, this would require more investigation and could be future work. 
Using low complexity models made the prediction time fast even when using the voting system.


\subsection{Validation}
\label{sec:discussion-validation}


Since classifications of the validation dataset was good with over 80\% accuracy, the dataset used for training was representative enough. 
The games, Wolcen and GTA V, are both similar and very different being in different sub-genres. 
However, both are games and reviews resemble one another but genre specific words are probably not included in training. 
The result could possibly be improved with a broader set of games used for training. 


The models performed otherwise as expected with majority voting having the highest accuracy, just as described in section \autoref{sec:classification-models}. 
Analysing the plots created had very similar result, since FP and FN is balancing cancelling each other when looking at the figures. 
It also looks like the misclassifications are distributed along the time axis, not causing any large difference in the overall shapes. 


As for the most common positive and negative words they are quite expected. 
Writing \emph{good}, \emph{great} or \emph{fun} in a positive review or \emph{bugs} in a negative review is probably common. 
It looks like \emph{just} is very common in negative reviews, which might not be expected. 
This is probably due to writing \emph{...just like *another game*..} or \emph{...just too many bugs...}, this analysis could also be future work.


The prediction made by the author in \autoref{cha:method} are quite accurate missing only the fact that the number of reviews are almost zero in-between the game release and release of the new content. 
At the time of the release of Wolcen the number of reviews are around 8000 positive and 6000 negative. 
Number of reviews increases just before release and start to decline fast after release down to a few reviews per month. 
Then the review numbers go up just before new content release, peaks just after the release and then goes down again. 
More reviews were predicted to happen during content release, but the difference in orders of magnitude was correctly captured. 


\section{Method}
\label{sec:discussion-method}


The training dataset was around five times as large as the validation set, this was at least in this application good enough based on the results. 
Training data could be from multiple games, some from a similar sub-genre might increase the performance of the model. 
Mining the data trough the Steam Web API was good in order to format the data at the same time to a usable format. 
This also ensured some consistency within the data, no missing data points etc. 


For this work the most simple text processing, term frequency, was used since it was good enough as seen in \autoref{cha:theory}. 
However, as the literature review conclude there are much that can be done to increase the accuracy by a considerable amount. 
With the modular approach this is possible and could also be used with majority voting, using different corpus features. 


It is similar with the models used, most of them commonly used in related works but there are suggestions that more complex models and parametrization would increase accuracy. 
Because of the modular design this is easy to implement and could also be used with majority voting. 
It was interesting to use KNN and see the results, even though it was not within the commonly used ML models for sentiment analysis. 


The evaluation made for the complete application on validation dataset was successful and the aim is completed. 
Using these kind of end-to-end applications does span over several large fields in terms of literature review and what to cover. 
Therefore it can be difficult to get any depth in a project with the time limit described in section \autoref{sec:delimitations}. 
At the same time the project is exposed to real world scenarios, combining the problem space from industry with academia. 
With the approach made there are several future work areas available within this package, allowing for much improvement.


A more structured approach for literature study could also change the method used in this report, which could also have changed the outcome. 
Since popular models were selected from the related research, if all referenced material would change the method would change as well. 
For a more stable method a larger literature study and as stated a more structured one could render a better quality foundation for this project.